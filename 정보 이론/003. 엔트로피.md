**엔트로피**는 확률 분포로부터 정보량을 측정하는 방법입니다. 이를 이해하기 위해 두 개의 이산 확률 변수, $X$와 $Y$가 그리스 문자에 대한 확률 분포를 가지는 상황을 살펴보겠습니다.

확률 변수 $X$는 다음과 같은 확률 분포 $p_X(x)$를 가집니다:

- $p_X(\alpha) = 0.25$
- $p_X(\beta) = 0.25$
- $p_X(\gamma) = 0.25$
- $p_X(\delta) = 0.25$

확률 변수 $Y$는 다음과 같은 확률 분포 $p_Y(y)$를 가집니다:

- $p_Y(\alpha) = 0.5$
- $p_Y(\beta) = 0.125$
- $p_Y(\gamma) = 0.125$
- $p_Y(\delta) = 0.25$

확률 변수 $X$의 경우, 각 문자를 구분하기 위해 필요한 최소 질문의 수는 2개입니다. 이는 처음에 (α, β)와 (γ, δ)로 나눠서 질문할 수 있기 때문입니다. 두 그룹의 확률이 각각 50%이므로, 효율적으로 질문할 수 있습니다.

반면, 확률 변수 $Y$에서는 α가 이미 50%의 확률로 나타나기 때문에, 처음에 "α입니까?"라고 묻는 것이 더 효율적입니다. 그 후, (β, γ, δ) 중에서 δ가 0.25의 확률을 차지하므로, "δ입니까?"라고 묻는 것이 적절합니다.

따라서, 확률 변수 $Y$에서 문자를 구별하기 위해 평균적으로 필요한 질문의 수는 다음과 같이 계산됩니다:

$$
p_Y(\alpha) \cdot 1 + p_Y(\beta) \cdot 3 + p_Y(\gamma) \cdot 3 + p_Y(\delta) \cdot 2 = 1.75
$$

이 계산은 각 문자가 발생할 확률에 그 문자를 결정하기 위한 질문의 수를 곱한 후, 그 값들을 합산하여 얻은 결과입니다. 확률 변수 Y에서 각 문자를 결정하기 위한 질문 횟수는 해당 문자의 발생 확률의 역수에 로그를 취한 값으로 계산됩니다.

- $\alpha$: $1 = \log_2(2) = \log_2(1/p_Y(\alpha)) = -\log_2 p_Y(\alpha)$
- $\beta$: $3 = \log_2(8) = \log_2(1/p_Y(\beta)) = -\log_2 p_Y(\beta)$
- $\gamma$: $3 = \log_2(8) = \log_2(1/p_Y(\gamma)) = -\log_2 p_Y(\gamma)$
- $\delta$: $2 = \log_2(4) = \log_2(1/p_Y(\delta)) = -\log_2 p_Y(\delta)$

Claude Shannon은 이러한 불확실성을 측정하는 방법을 '엔트로피(Entropy)'라고 명명했습니다. 엔트로피는 주어진 확률 분포에서 사건을 예측하는 데 필요한 평균 질문의 수를 측정합니다. Shannon은 이를 $H$로 표기하며, 단위는 비트(bit)입니다. 확률 변수 $X$의 엔트로피 $H_{p_X}(X)$는 다음과 같이 계산됩니다:

$$
H_{p_X}(X) = -\sum_{x \in X} p_X(x) \log_2 p_X(x)
$$

여기서 $p_X(x)$는 사건 $x$가 발생할 확률입니다. $H_{p_X}(X)$는 엔트로피가 확률 분포 $p_X$에 의존하며, 랜덤 변수 $X$와 관련이 있음을 나타냅니다. 사건의 확률의 역수에 로그를 취한 값은, 사건 발생 확률이 낮을수록 더 많은 정보를 제공한다는 것을 의미합니다.

엔트로피는 모든 사건이 동일한 확률로 발생할 때 최댓값을 가집니다. 이는 불확실성이 가장 크다는 것을 의미합니다. 반대로, 특정 사건이 더 높은 확률로 발생하면, 엔트로피는 감소합니다. 이는 사건을 예측하는 데 필요한 평균 질문의 수가 줄어든다는 것을 의미하며, 정보의 불확실성이 줄어들었음을 나타냅니다. 앞의 예시에서 확률 변수 X는 100개의 글자를 맞히기 위해 200번의 질문이 필요했지만, 확률 변수 Y는 175번의 질문만으로 충분했습니다. 이는 확률 변수 Y가 X보다 더 적은 정보량을 생산한다고 볼 수 있습니다. 엔트로피는 가능한 모든 사건이 동일한 확률로 발생할 때 최대가 되며, 확률이 조금만 달라져도 엔트로피는 감소합니다.

