크로스 엔트로피는 주로 실제 확률 분포와 예측 확률 분포 간의 차이를 측정하여, 두 분포 간의 정보 손실을 정량화하는 방법 중 하나로 사용됩니다. 이는 특히 머신 러닝과 정보 이론에서 예측 성능을 평가하는 중요한 도구입니다.

이전에 설명한 바와 같이, **엔트로피 값은 확률 변수의 실제 확률 분포를 사용할 때 예상되는 질문의 평균 개수를 나타냅니다**. 그렇다면 실제 분포가 아닌 다른 예측 분포를 사용할 경우, 이 예측이 실제 분포와 얼마나 차이가 있을까요?

크로스 엔트로피는 주어진 예측 확률 분포가 실제 확률 분포와 얼마나 일치하는지를 평가하는 데 사용됩니다. 예를 들어, 앞선 예제의 확률 변수 $X$의 확률 분포 $p_X$가, 확률 변수 $Y$의 확률 분포 $p_Y$와 같이 $\alpha$, $\beta$, $\gamma$, $\delta$ 옵션 각각에 대해 각각 0.5, 0.125, 0.125, 0.25의 확률로 구성되어 있다고 가정할 수 있습니다. 이 경우, 크로스 엔트로피는 $p_Y$가 $p_X$​를 얼마나 잘 예측하는지를 수량화합니다.

$p_Y$의 지지 집합이 $p_X$의 지지 집합을 포함할 때, 크로스 엔트로피는 다음과 같이 정의됩니다:
$$
H_{p_X \parallel p_Y}(X) = -\sum_{x \in X} p_X(x) \log_2 p_Y(x)
$$

여기서:
- $p_X(x)$는 실제 분포에서 사건 $x$가 발생할 확률입니다.
- $p_Y(x)$는 예측 분포에서 사건 $x$에 할당된 확률입니다.
- 합은 가능한 모든 사건 $x$에 대해 수행됩니다.

$p_Y$의 지지 집합이 $p_X$의 지지 집합을 포함하지 않을 때, 즉 어떤 지점에서 $p_X(x)>0$이지만 $p_Y(x)=0$인 경우, $H_{p_X \parallel p_Y}(X)$는 그 지점에서 정의되지 않습니다. 이는 로그 함수의 특성 때문인데, $log⁡(0)$은 정의되지 않기 때문입니다. 실제로 계산할 때 이러한 문제를 피하기 위해 $log(0)$을 매우 큰 음수 값으로 설정하는 방법을 사용할 수 있습니다. 또한 매우 작은 값에 대해 로그를 취할 때 발생할 수 있는 수치적 불안정성을 보완하기 위해, 특정 임계값 이하의 값을 적절한 작은 값으로 대체하는 방법도 사용할 수 있습니다.

예를 들어, 위 예제에서 크로스 엔트로피를 계산한 결과는 다음과 같습니다:
$$H_{p_X \parallel p_Y}(X)=0.25 \cdot 1 + 0.25 \cdot 3 + 0.25 \cdot 3 + 0.25 \cdot 2 = 2.25$$
이 크로스 엔트로피 값은 예측 분포 $p_Y$가 실제 분포 $p_X$를 얼마나 비효율적으로 예측하고 있는지를 나타냅니다. 값이 높을수록 예측이 실제와 더 큰 차이가 있음을 의미합니다.

예측 분포가 실제 분포와 더 가까워질수록 크로스 엔트로피 값은 낮아지기 때문에, 크로스 엔트로피는 머신 러닝 모델의 예측 성능을 평가하는 데 자주 사용됩니다. 특히 분류 문제에서, 모델이 예측한 클래스 확률 분포가 실제 클래스 분포와 얼마나 일치하는지를 측정합니다. 낮은 크로스 엔트로피 값은 모델이 실제 분포에 가까운 예측을 하고 있음을 나타냅니다.

크로스 엔트로피는 정보 획득의 관점에서도 이해될 수 있습니다. 예측 분포가 실제 분포와 다를수록, 우리는 더 많은 정보를 필요로 하게 됩니다. 따라서 크로스 엔트로피 값이 높다는 것은 정보 획득 비용이 증가한다는 것을 의미합니다.
