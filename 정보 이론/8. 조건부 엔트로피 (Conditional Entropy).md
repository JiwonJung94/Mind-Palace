조건부 엔트로피는 두 확률 변수 중 하나가 주어졌을 때, 다른 하나의 불확실성을 측정하는 개념입니다. 조건부 엔트로피는 **두 변수 사이의 종속성을 나타내는 지표**로 사용됩니다. 이는 특정 조건 하에서의 불확실성을 이해하는 데 중요한 역할을 합니다. 

두 이산 확률 변수 $X$와 $Y$에 대해 $Y$가 주어졌을 때 $X$의 조건부 엔트로피 $H_{p_{X|Y}}(X|Y)$는 다음과 같이 정의됩니다:

$$
H_{p_{X|Y}}(X|Y) = -\sum_{y \in Y} p_Y(y) \sum_{x \in X} p_{X|Y}(x|y) \log_2 p_{X|Y}(x|y)
$$

여기서:
- $p_{X|Y}(x|y)$는 $Y = y$일 때 $X = x$가 될 조건부 확률입니다.
- $p_Y(y)$는 $Y$가 $y$의 값을 가질 확률입니다.

조건부 확률은 결합 확률과 개별 확률을 통해 다음과 같은 관계로 정의됩니다. 두 확률 변수 $X$와 $Y$에 대해, $Y = y$일 때 $X = x$가 될 조건부 확률 $p_{X|Y}(x|y)$는 결합 확률 $p_{X,Y}(x,y)$와 $Y$의 개별 확률 $p_Y(y)$를 사용하여 다음과 같이 계산됩니다:

$$
p_{X|Y}(x|y) = \frac{p_{X,Y}(x,y)}{p_Y(y)}
$$

여기서:
- $p_{X,Y}(x,y)$는 $X$와 $Y$가 각각 $x$와 $y$의 값을 동시에 가질 결합 확률입니다.
- $p_Y(y)$는 $Y$가 $y$의 값을 가질 확률입니다.

이 관계는 조건부 확률의 정의에서 비롯되며, 특정 사건 $Y = y$가 발생했을 때 $X = x$가 발생할 확률을 계산하는 방법을 제공합니다. 이 식은 $p_Y(y) \neq 0$인 경우에만 유효합니다.

조건부 확률의 정의에 따라, 조건부 엔트로피는 다음과 같이 표현됩니다:

$$
H_{p_{X|Y}}(X|Y) = -\sum_{y \in Y} p_Y(y) \sum_{x \in X} \left( \frac{p_{X,Y}(x,y)}{p_Y(y)} \right) \log_2 \left( \frac{p_{X,Y}(x,y)}{p_Y(y)} \right)
$$
이 식을 정리하면:
$$
H_{p_{X|Y}}(X|Y) = -\sum_{x \in X} \sum_{y \in Y} p_{X,Y}(x,y) \left( \log_2 p_{X,Y}(x,y) - \log_2 p_Y(y) \right)
$$
이를 두 항으로 나누면:
$$
H_{p_{X|Y}}(X|Y) = -\sum_{x \in X} \sum_{y \in Y} p_{X,Y}(x,y) \log_2 p_{X,Y}(x,y) + \sum_{x \in X} \sum_{y \in Y} p_{X,Y}(x,y) \log_2 p_Y(y)
$$

첫 번째 항은 결합 엔트로피의 정의와 일치합니다:
$$
-\sum_{x \in X} \sum_{y \in Y} p_{X,Y}(x,y) \log_2 p_{X,Y}(x,y) = H_{p_{X,Y}}(X, Y)
$$
두 번째 항을 보면:
$$
\sum_{x \in X} \sum_{y \in Y} p_{X,Y}(x,y) \log_2 p_Y(y) = \sum_{y \in Y} p_Y(y) \log_2 p_Y(y) = -H_{p_Y}(Y)
$$

따라서, 조건부 엔트로피는 다음과 같이 결합 엔트로피와 개별 엔트로피로 표현됩니다.
$$
H_{p_{X|Y}}(X|Y) = H_{p_{X,Y}}(X, Y) - H_{p_Y}(Y)
$$

이 식은 조건부 엔트로피가 결합 엔트로피에서 $Y$의 불확실성을 제거한 것임을 보여줍니다. 다시 말해, 조건부 엔트로피는 $Y$의 정보가 주어졌을 때 $X$의 불확실성이 얼마나 감소하는지를 측정합니다. 만약 $Y$가 $X$에 대해 많은 정보를 제공한다면, 조건부 엔트로피는 낮아질 것입니다.

두 확률 변수 $X$와 $Y$가 독립적일 때, $p_{X,Y}​(x,y)=p_X(x) \cdot p_Y(y)$가 성립하므로 조건부 확률 $p_{X|Y}(x|y)$는 다음과 같이 단순화됩니다.
$$
p_{X|Y}(x|y) = p_X(x)
$$

이는 $Y$의 값이 무엇이든 간에 $X$의 확률이 변하지 않는다는 것을 나타냅니다. 즉, $X$와 $Y$가 독립적일 때, $X$의 조건부 확률은 $X$의 개별 확률과 동일합니다.

두 확률 변수 $X$와 $Y$가 독립적인 경우, $p_{X|Y}(x|y) = p_X(x)$이므로 조건부 엔트로피 $H_{p_{X|Y}}(X|Y)$는 다음과 같이 단순화됩니다:
$$
H_{p_{X|Y}}(X|Y) = -\sum_{y \in Y} p_Y(y) \sum_{x \in X} p_X(x) \log_2 p_X(x)
$$
위 식에서 내부 합은 $y$와 무관하므로, 외부 합이 단순히 $p_Y(y)$의 합이 되어 1이 됩니다:

$$
H_{p_{X|Y}}(X|Y) = -\sum_{x \in X} p_X(x) \log_2 p_X(x) \sum_{y \in Y} p_Y(y)
$$
$$
H_{p_{X|Y}}(X|Y) = -\sum_{x \in X} p_X(x) \log_2 p_X(x)
$$

이는 바로 $X$의 엔트로피 $H_{p_X}(X)$입니다:
$$
H_{p_{X|Y}}(X|Y) = H_{p_X}(X)
$$
$X$와 $Y$가 독립적일 때, $Y$가 제공하는 정보가 $X$의 불확실성을 줄이지 않으므로 조건부 엔트로피 $H_{p_{X|Y}}(X|Y)$는 $X$의 엔트로피 $H_{p_X}(X)$와 동일하게 됩니다. 이는 $Y$가 $X$에 대해 아무런 추가 정보를 제공하지 않는 상황을 반영합니다. 반면, $X$와 $Y$가 의존적이라면, $H(X|Y) < H(X)$가 될 수 있습니다.

















