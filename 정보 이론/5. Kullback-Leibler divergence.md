Kullback-Leibler 발산(KL 발산)은 두 확률 분포 간의 차이를 정량화하는 데 사용되는 개념입니다. 이는 주어진 예측 분포 $p_Y$가 실제 분포 $p_X$와 얼마나 다른지를 측정합니다. KL 발산은 정보 이론에서 불확실성이나 정보 손실을 평가하는 데 유용한 도구로 사용됩니다.

$p_Y$의 지지 집합이 $p_X$의 지지 집합을 포함할 때, KL 발산은 다음과 같이 정의됩니다:

$$
D^{KL}_{p_X \parallel p_Y}(X) = \sum_{x \in X} p_X(x) \log_2 \frac{p_X(x)}{p_Y(x)}
$$

여기서:
- $p_X(x)$는 실제 분포에서 사건 $x$가 발생할 확률입니다.
- $p_Y(x)$는 예측 분포에서 사건 $x$에 할당된 확률입니다.
- 합은 가능한 모든 사건 $x$에 대해 수행됩니다.

$p_Y$의 지지 집합이 $p_X$의 지지 집합을 포함하지 않을 때, 즉 어떤 지점에서 $p_X(x)>0$이지만 $p_Y(x)=0$인 경우, $D^{KL}_{p_X \parallel p_Y}(X)$는 그 지점에서 정의되지 않습니다. 이는 로그 함수의 특성 때문인데, $log⁡(0)$은 정의되지 않기 때문입니다. 실제로 계산할 때 이러한 문제를 피하기 위해 $log(0)$을 매우 큰 음수 값으로 설정하는 방법을 사용할 수 있습니다. 또한 매우 작은 값에 대해 로그를 취할 때 발생할 수 있는 수치적 불안정성을 보완하기 위해, 특정 임계값 이하의 값을 적절한 작은 값으로 대체하는 방법도 사용할 수 있습니다.

각 항목 $p_X(x) \log_2 \frac{p_X(x)}{p_X(x)}$는 사건 $x$가 실제로 발생할 확률 $p_X(x)$와 예측된 확률 $p_Y(x)$ 간의 차이를 측정합니다. 

정보 이론의 관점에서 해석하면, $-\log_2 p_X(x)$는 사건 $x$가 실제로 발생했을 때 얻는 정보량입니다. 반면, $-\log_2 p_Y(x)$는 예측된 분포를 기반으로 사건 $x$가 발생했을 때 기대되는 정보량입니다. 따라서, $\log_2 \frac{p_X(x)}{p_Y(x)} = \log_2 p_X(x) - \log_2 p_Y(x)$는 예측과 실제 간의 정보량 차이를 나타냅니다.

이 차이를 $p_X(x)$로 가중합한 KL 발산은, 실제로 사건이 발생했을 때 예측 분포를 사용함으로써 발생하는 평균적인 정보 손실을 나타냅니다. 즉, KL 발산은 예측 분포가 실제 분포와 얼마나 다른지를 통해, 예측이 얼마나 비효율적인지를 수치적으로 보여줍니다.

KL 발산을 크로스 엔트로피와 엔트로피의 차이로 표현하면 이러한 의미를 더욱 명확하게 이해할 수 있습니다.
$$
\begin{align*}
D^{KL}_{p_X \parallel p_Y}(X) &= \sum_{x \in X} p_X(x) \log_2 \frac{p_X(x)}{p_Y(x)} \\
&= \sum_{x \in X} p_X(x) \left( \log_2 p_X(x) - \log_2 p_Y(x) \right) \\
&= \sum_{x \in X} p_X(x) \log_2 p_X(x) - \sum_{x \in X} p_X(x) \log_2 p_Y(x) \\
&= -H_{p_X}(X) + H_{p_X \parallel p_Y}(X) \\
&= H_{p_X \parallel p_Y}(X) - H_{p_X}(X)
\end{align*}

$$

즉, KL 발산은 크로스 엔트로피에서 실제 분포의 엔트로피를 뺀 값으로 정의됩니다. 크로스 엔트로피 $H(P,Q)$는 예측 분포 $Q$를 사용할 때의 평균적인 정보량을 측정하고, 엔트로피 $H(P)$는 실제 분포 자체의 불확실성을 측정하므로, KL 발산은 예측 분포 $Q$를 사용할 때 추가적으로 발생하는 정보 손실을 나타내며, 이는 예측의 비효율성을 정량화합니다. KL 발산의 값이 0이면, 두 분포가 동일하다는 것을 의미합니다. 값이 클수록 두 분포 간의 차이가 크다는 것을 나타내며, 이는 예측 분포 $Q$가 실제 분포 $P$를 잘 설명하지 못한다는 것을 의미합니다.

이러한 특성 때문에 KL 발산은 최적화 문제에서 손실 함수로 사용되기도 합니다. 모델이 학습하는 동안 KL 발산을 최소화함으로써, 모델의 예측 분포가 실제 데이터 분포에 점점 더 가까워지도록 유도할 수 있습니다.