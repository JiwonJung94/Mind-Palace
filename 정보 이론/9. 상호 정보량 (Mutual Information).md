상호 정보량(Mutual Information)은 두 확률 변수 간의 의존성을 측정하는 중요한 개념입니다. 이는 두 변수 간의 정보 공유 정도를 나타내며, 결합 엔트로피와 조건부 엔트로피를 활용하여 정의할 수 있습니다.

### 상호 정보량의 정의

두 이산 확률 변수 $X$와 $Y$에 대한 상호 정보량 $I(X; Y)$는 다음과 같이 정의됩니다:

$$
I(X; Y) = H_{p_X}(X) - H_{p_{X|Y}}(X|Y)
$$

여기서:
- $H_{p_X}(X)$는 $X$의 개별 엔트로피로, $X$의 불확실성을 나타냅니다.
- $H_{p_{X|Y}}(X|Y)$는 $Y$가 주어졌을 때 $X$의 조건부 엔트로피로, $Y$의 정보가 제공된 후 $X$의 남은 불확실성을 나타냅니다.

상호 정보량은 $X$의 전체 불확실성에서 $Y$가 제공하는 정보를 통해 줄어든 불확실성을 측정합니다.

### 상호 정보량과 결합 엔트로피의 관계

상호 정보량은 결합 엔트로피를 사용하여 다음과 같이 표현할 수도 있습니다:

$$
I(X; Y) = H_{p_X}(X) + H_{p_Y}(Y) - H_{p_{X,Y}}(X, Y)
$$

여기서:
- $H_{p_{X,Y}}(X, Y)$는 $X$와 $Y$의 결합 엔트로피로, 두 변수의 결합 확률 분포에 대한 전체적인 불확실성을 나타냅니다.

이 식은 상호 정보량이 두 변수의 개별 엔트로피 합에서 결합 엔트로피를 뺀 값임을 보여줍니다. 이는 두 변수가 공유하는 정보의 양을 나타냅니다.

### 상호 정보량의 성질

1. **비대칭성**: $I(X; Y) = I(Y; X)$. 상호 정보량은 두 변수의 순서와 무관합니다.
   
2. **비음수성**: $I(X; Y) \geq 0$. 상호 정보량은 항상 0 이상입니다. $I(X; Y) = 0$일 때, 두 변수는 독립적입니다.

3. **상호 정보량과 독립성**: 두 변수가 독립적이라면, 상호 정보량은 0이 됩니다. 이는 두 변수가 서로에 대해 아무런 정보를 제공하지 않는다는 것을 의미합니다.

### 상호 정보량의 의미

상호 정보량은 두 변수 간의 상호 의존성을 정량화하는 데 사용됩니다. 이는 다음과 같은 다양한 상황에서 유용합니다:

- **피처 선택**: 머신러닝에서 상호 정보량은 피처가 타겟 변수에 대해 얼마나 많은 정보를 제공하는지를 평가하는 데 사용됩니다. 높은 상호 정보량을 가진 피처는 모델 성능을 향상시킬 가능성이 큽니다.

- **통신 시스템**: 상호 정보량은 채널 용량을 평가하는 데 사용됩니다. 이는 채널을 통해 얼마나 많은 정보를 효율적으로 전송할 수 있는지를 나타냅니다.

- **생물정보학**: 유전자 간의 상호작용을 분석하거나, 단백질 구조의 상관관계를 연구하는 데 상호 정보량이 활용됩니다.
